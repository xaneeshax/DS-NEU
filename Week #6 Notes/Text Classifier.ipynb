{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2> Text Classifiers</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Outline\n",
    "1. <a href='#1'>Logistic Regression</a>\n",
    "2. <a href='#2'>Multilayer Perceptron Classifier</a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Logistic Regression\n",
    "* Despite its name, a classification algorithm\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Used for binary classification - very popular for sentiment analysis\n",
    "\n",
    "y_hat = logistic(m1x1 + m2x2 +... + mnxn + b)\n",
    "\n",
    "\n",
    "Sigmoid Function\n",
    "- Compresses the linear function so that is limited to the range [0,1]\n",
    "- The output value is interpreted as the conditional probability \n",
    "  of the input instances belonging to the positive class, given its features\n",
    "  \n",
    "  y_hat = 1 / 1 + exp[ -(m1x1 + m2x2 + ... + mnxn + b)]\n",
    "  \n",
    "Similar to Ridge Regression as it is a linear model and uses L2 Regularization\n",
    "\n",
    "Regularization Parameter: c\n",
    "Higher values of C increases the complexity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"game_review.csv\")\n",
    "features = data[\"comment\"]\n",
    "target = data[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy on training set:  0.9012009495880463\n",
      "Classification accuracy on testing set:  0.8217801047120419\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=3000)\n",
    "\n",
    "\n",
    "#create the vocabulary based on the training data\n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "#encode the words in X_train and X_test based on the vocabulary\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "#train the classifier\n",
    "model = LogisticRegression().fit(X=X_train_vectorized, y=y_train)\n",
    "\n",
    "\n",
    "print(\"Classification accuracy on training set: \", model.score(X_train_vectorized, y_train))\n",
    "print(\"Classification accuracy on testing set: \", model.score(X_test_vectorized, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Probability Estimates\n",
    "* possible to get probability estimates for predictions\n",
    "* use model.predict_proba()\n",
    "    * Returned estimates for all classes are ordered by the label of classes.\n",
    "\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74943183, 0.25056817],\n",
       "       [0.07551562, 0.92448438],\n",
       "       [0.27700778, 0.72299222],\n",
       "       ...,\n",
       "       [0.49807486, 0.50192514],\n",
       "       [0.58939542, 0.41060458],\n",
       "       [0.65454571, 0.34545429]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = pd.DataFrame(model.predict_proba(X_test_vectorized), columns = ['Not Recommended', 'Recommended'])\n",
    "proba['prediction'] = model.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Not Recommended</th>\n",
       "      <th>Recommended</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.749432</td>\n",
       "      <td>0.250568</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.075516</td>\n",
       "      <td>0.924484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.277008</td>\n",
       "      <td>0.722992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.590125</td>\n",
       "      <td>0.409875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111302</td>\n",
       "      <td>0.888698</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.728571</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>0.633448</td>\n",
       "      <td>0.366552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4772</th>\n",
       "      <td>0.498075</td>\n",
       "      <td>0.501925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4773</th>\n",
       "      <td>0.589395</td>\n",
       "      <td>0.410605</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4774</th>\n",
       "      <td>0.654546</td>\n",
       "      <td>0.345454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4775 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Not Recommended  Recommended  prediction\n",
       "0            0.749432     0.250568           0\n",
       "1            0.075516     0.924484           1\n",
       "2            0.277008     0.722992           1\n",
       "3            0.590125     0.409875           0\n",
       "4            0.111302     0.888698           1\n",
       "...               ...          ...         ...\n",
       "4770         0.271429     0.728571           1\n",
       "4771         0.633448     0.366552           0\n",
       "4772         0.498075     0.501925           1\n",
       "4773         0.589395     0.410605           0\n",
       "4774         0.654546     0.345454           0\n",
       "\n",
       "[4775 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multilayer Perceptron Classifier\n",
    "* Simple, feed-forward neural network\n",
    "* hidden_layer_sizes = (10) adds a single hidden layer with 10 hidden units\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Neural Networks\n",
    "\n",
    "A family of algorithms modeled loosely after the human brain\n",
    "If the signal exceeds a certain threshold then the sequence is generated (parallels with a logic gate)\n",
    "\n",
    "Multilayer Perceptrons\n",
    "- Simple Feed-forward Neural Network aka Vanilla\n",
    "- Use a reductionist approach to mimic how the brain works\n",
    "- The neuron either fires or it doesn't\n",
    "\n",
    "Nueron Activation\n",
    "- Node that represents a feature value\n",
    "- If net input function value is equal to the threshhold, there will be an output value\n",
    "\n",
    "\n",
    "Hidden Layer\n",
    "- Multiple hidden units\n",
    "- compute the units that are an intermediate processing step\n",
    "\n",
    "    y_hat = v1h1 + v2h2 + v3h3\n",
    "    \n",
    "Activation Function\n",
    "- Map the summed weighted inputs to the output of the neuron\n",
    "- govern the threshold where the neuron is activated\n",
    "- strength of the output signal\n",
    "- relu function\n",
    "- we can map values from 0 - infintiy\n",
    "\n",
    "hi = tanh( m1ix1 + m2ix2 + ... + mnixn)\n",
    "Takeaway: each node or feature has its own weight\n",
    "\n",
    "Deep Learning\n",
    "- Inspired by the idea of having large neural networks made up of many hidden layers of computation\n",
    "- May be computationally heavy due to the number of hidden layers and calculations that need to be made\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy on training set:  0.9973467392822232\n",
      "Classification accuracy on testing set:  0.8098429319371728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=3000)\n",
    "\n",
    "\n",
    "#create the vocabulary based on the training data\n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "#encode the words in X_train and X_test based on the vocabulary\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "#train the classifier\n",
    "# solvers\n",
    "# lbfgs : an optimizer on the family of quasi-Newton Methods\n",
    "# sgd : refers to the stochastic gradient descent\n",
    "# adam : refers to a stochastic gradient-based optimizer\n",
    "\n",
    "model = MLPClassifier(solver = 'lbfgs', hidden_layer_sizes = (10), activation = 'logistic',\n",
    "                      random_state = 3000).fit(X=X_train_vectorized, y=y_train)\n",
    "\n",
    "\n",
    "print(\"Classification accuracy on training set: \", model.score(X_train_vectorized, y_train))\n",
    "print(\"Classification accuracy on testing set: \", model.score(X_test_vectorized, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.19149116, -0.08667255, -0.11229894, ..., -0.12139146,\n",
       "          0.33095417,  0.20810238],\n",
       "        [ 0.08775012, -0.09972273, -0.14690432, ..., -0.11743543,\n",
       "          0.26695724,  0.15056969],\n",
       "        [ 0.0098364 , -0.00665959, -0.01558428, ..., -0.02525612,\n",
       "          0.02521357,  0.01154272],\n",
       "        ...,\n",
       "        [ 0.03256535,  0.01780136,  0.03735015, ...,  0.03451751,\n",
       "          0.03644553,  0.00166909],\n",
       "        [-0.0778984 ,  0.07554666,  0.09826078, ...,  0.09402758,\n",
       "         -0.16705507, -0.13957366],\n",
       "        [ 0.08251011, -0.04873931, -0.07672365, ..., -0.05703895,\n",
       "          0.14424234,  0.09660233]]),\n",
       " array([[-0.63120187],\n",
       "        [ 1.27958601],\n",
       "        [ 1.92305251],\n",
       "        [ 1.6750819 ],\n",
       "        [-0.87286191],\n",
       "        [ 1.85710689],\n",
       "        [ 1.85462438],\n",
       "        [ 1.61090875],\n",
       "        [-0.52948209],\n",
       "        [-6.01310613]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coefs_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TUNING NEURAL NETWORKS- Tuning\n",
    "\n",
    "hidden_layer_sizes: sets the number of hidden layers and number of hidden units per layer\n",
    "\n",
    "alpha: controls the weight of the regularization penalty that shrinks the weight to 0\n",
    "\n",
    "activation: controls the nonlinear function used for the activation function including\n",
    "            'relu', 'logistic', 'tanh'"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
