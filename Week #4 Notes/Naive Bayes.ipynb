{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A probabalisitc classification algorithm\n",
    "Estimate the probability of class labels based on certain features\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "It's called Naive because it assumes each of these features\n",
    "are equally important or a priori\n",
    "\n",
    "Second, it assumes that these features are statistically independent\n",
    "The Naive Bayes algorithm assumes for example x1 and x2 are completely independant\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Variations of Naive Bayes Algorithms\n",
    "\n",
    "1. Bernoulli: for binary features\n",
    "\n",
    "2. Multinomal: for discrete features\n",
    "\n",
    "3. Gaussian: for continuous features\n",
    "    - Kinda like a density plot\n",
    "    - Larger probability toward the center of the ellipses\n",
    "    - Uses the mean and sd for each group\n",
    "    - The class with the highest probability 'wins'\n",
    "    - Really easy and fast to classify\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pros of Gaussian Naive-Bayes\n",
    "- Easy to understand\n",
    "- Simple and efficient parameter estimation\n",
    "- Works well with high-dimensional data\n",
    "- Often useful for baseline comparision against more sophisticated methods\n",
    "\n",
    "Cons of Gaussian Naive-Bayes\n",
    "- Assumption that features are conditionally independant is naive\n",
    "- Other classifiers often generalize better: usually have more dependant variables when working with fewer groups\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "df = pd.DataFrame(digits.data)\n",
    "df['target'] = digits.target\n",
    "\n",
    "features = df.drop('target', axis = 1)\n",
    "target = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy on the test data: 85.33%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state = 3000)\n",
    "\n",
    "# select a classifier and create the model by fitting the training data\n",
    "gnb = GaussianNB().fit(X = X_train, y = y_train)\n",
    "\n",
    "#make predictions on the test set\n",
    "predicted = gnb.predict(X = X_test)\n",
    "\n",
    "# Prediction Accuracy\n",
    "accuracy = gnb.score(X_test, y_test)\n",
    "print('Prediction Accuracy on the test data:', f'{accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
